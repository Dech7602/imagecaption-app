# -*- coding: utf-8 -*-
"""caption&segmentation.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1w_yO2HxjM7PRNJrdJ8BiaT01NWXvFe1S
"""

!wget http://images.cocodataset.org/zips/train2017.zip -O coco_train2017.zip
!wget http://images.cocodataset.org/zips/val2017.zip -O coco_val2017.zip
!wget http://images.cocodataset.org/annotations/annotations_trainval2017.zip -O coco_ann2017.zip

from zipfile import ZipFile, BadZipFile
import os

def extract_zip_file(extract_path):
    try:
        # The extract_path already contains the .zip extension, so remove the concatenation.
        with ZipFile(extract_path) as zfile:
            # We want to extract to the parent directory of the zip file, not a directory with the zip file's name.
            # Get the directory part of the path.
            extract_directory = os.path.dirname(extract_path)
            zfile.extractall(extract_directory)

        # remove zipfile
        zfileTOremove=f"{extract_path}"
        if os.path.isfile(zfileTOremove):
            os.remove(zfileTOremove)
        else:
            print("Error: %s file not found" % zfileTOremove)

    except BadZipFile as e:
        print("Error:", e)


extract_train_path = "/content/coco_train2017.zip"
extract_val_path = "/content/coco_val2017.zip"
extract_ann_path="/content/coco_ann2017.zip"

extract_zip_file(extract_train_path)
extract_zip_file(extract_val_path)
extract_zip_file(extract_ann_path)

!pip install pycocotools



import json
import random
import matplotlib.pyplot as plt
from PIL import Image
import numpy as np
import os
import cv2

from pycocotools.coco import COCO
from matplotlib.patches import Polygon
from matplotlib.collections import PatchCollection

# Paths
image_dir = "/content/train2017"
caption_ann_file = "/content/annotations/captions_train2017.json"
seg_ann_file = "/content/annotations/instances_train2017.json"

# Load COCO annotations
coco_caps = COCO(caption_ann_file)
coco_segs = COCO(seg_ann_file)

# Get a random image ID that is common in both caption and segmentation files
image_ids = list(set(coco_caps.getImgIds()) & set(coco_segs.getImgIds()))
image_id = random.choice(image_ids)

# Load image metadata
image_info = coco_caps.loadImgs(image_id)[0]
image_path = os.path.join(image_dir, image_info['file_name'])
image = Image.open(image_path).convert('RGB')

# Load captions
caption_ids = coco_caps.getAnnIds(imgIds=image_id)
captions = coco_caps.loadAnns(caption_ids)
caption_text = "\n".join([ann['caption'] for ann in captions])

# Load segmentation masks
ann_ids = coco_segs.getAnnIds(imgIds=image_id)
anns = coco_segs.loadAnns(ann_ids)

# Display image with segmentation and captions
plt.figure(figsize=(10, 10))
plt.imshow(image)
plt.axis('off')

for ann in anns:
    if 'segmentation' in ann:
        if isinstance(ann['segmentation'], list):  # Polygon format
            for seg in ann['segmentation']:
                poly = np.array(seg).reshape((len(seg) // 2, 2))
                # Pass the 'closed' argument as a keyword and styling arguments as keywords
                polygon = Polygon(poly, closed=True, facecolor='none', edgecolor='red', linewidth=2)
                plt.gca().add_patch(polygon)


# Show the captions as a title
plt.title("Captions:\n" + caption_text, fontsize=12)
plt.tight_layout()
plt.show()

!pip install torch torchvision

import torch
import torch.nn as nn
import math

class PositionalEncoding(nn.Module):
    def __init__(self, d_model, max_len=5000):
        super(PositionalEncoding, self).__init__()
        pe = torch.zeros(max_len, d_model)
        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)
        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))
        pe[:, 0::2] = torch.sin(position * div_term)
        pe[:, 1::2] = torch.cos(position * div_term)
        pe = pe.unsqueeze(0)
        self.register_buffer('pe', pe)

    def forward(self, x):
        x = x + self.pe[:, :x.size(1), :]
        return x

from torchvision import models

class ImageCaptioningTransformer(nn.Module):
    def __init__(self, vocab_size, embed_dim=512, num_heads=8, num_layers=6, dropout=0.1, max_len=50):
        super(ImageCaptioningTransformer, self).__init__()

        # CNN Encoder (ResNet50 without classifier)
        resnet = models.resnet50(pretrained=True)
        modules = list(resnet.children())[:-2]
        self.cnn_encoder = nn.Sequential(*modules)
        self.adaptive_pool = nn.AdaptiveAvgPool2d((14, 14))
        self.linear_proj = nn.Linear(2048, embed_dim)

        # Positional encodings
        self.pos_encoder = PositionalEncoding(embed_dim, max_len)

        # Transformer decoder
        decoder_layer = nn.TransformerDecoderLayer(d_model=embed_dim, nhead=num_heads, dropout=dropout)
        self.transformer_decoder = nn.TransformerDecoder(decoder_layer, num_layers=num_layers)

        # Caption decoder
        self.embed = nn.Embedding(vocab_size, embed_dim)
        self.out = nn.Linear(embed_dim, vocab_size)

    def forward(self, images, tgt_seq):
        # Encode image
        features = self.cnn_encoder(images)                     # [B, 2048, H, W]
        features = self.adaptive_pool(features).flatten(2).permute(2, 0, 1)  # [S, B, 2048]
        features = self.linear_proj(features)                   # [S, B, D]

        # Prepare target input
        tgt_embed = self.embed(tgt_seq).permute(1, 0, 2)        # [T, B, D]
        tgt_embed = self.pos_encoder(tgt_embed)

        # Transformer decoding
        tgt_mask = nn.Transformer.generate_square_subsequent_mask(tgt_embed.size(0)).to(tgt_embed.device)
        output = self.transformer_decoder(tgt_embed, features, tgt_mask=tgt_mask)
        output = self.out(output.permute(1, 0, 2))              # [B, T, vocab_size]

        return output

import torchvision
from torchvision.datasets import CocoDetection
from torchvision.transforms import ToTensor
from torchvision.models.detection import maskrcnn_resnet50_fpn
from torch.utils.data import DataLoader
import torchvision.transforms as T
import torch
from pycocotools.coco import COCO

# Paths
img_dir = '/content/train2017'
ann_file = '/content/annotations/instances_train2017.json'

# Transform
transform = T.Compose([
    T.ToTensor()
])

class CocoSegmentationDataset(CocoDetection):
    def __getitem__(self, idx):
        img, targets = super().__getitem__(idx)
        boxes, labels, masks = [], [], []

        for obj in targets:
            if obj['iscrowd']:
                continue
            x, y, w, h = obj['bbox']
            if w <= 0 or h <= 0:
                continue  # invalid box, skip
            boxes.append([x, y, x + w, y + h])
            labels.append(obj['category_id'])
            masks.append(coco.annToMask(obj))

        if len(boxes) == 0:
            return self.__getitem__((idx + 1) % len(self))  # get next if empty

        boxes = torch.tensor(boxes, dtype=torch.float32)
        labels = torch.tensor(labels)
        masks = torch.stack([torch.tensor(m, dtype=torch.uint8) for m in masks])

        target = {
            'boxes': boxes,
            'labels': labels,
            'masks': masks,
            'image_id': torch.tensor([targets[0]['image_id']])
        }
        return T.ToTensor()(img), target

def run_dual_task(image_tensor, caption_model, segmentation_model, vocab):
    caption_model.eval()
    segmentation_model.eval()
    image = image_tensor.unsqueeze(0).to(device)

    # --- Caption Prediction ---
    with torch.no_grad():
        caption_output = caption_model(image, tgt_seq=torch.zeros(1, max_len, dtype=torch.long).to(device))
        predicted_ids = caption_output.argmax(2)[0]
        predicted_caption = ' '.join([vocab.itos[idx] for idx in predicted_ids if idx not in [vocab.stoi['<pad>'], vocab.stoi['<start>'], vocab.stoi['<end>']]])

    # --- Segmentation Prediction ---
    with torch.no_grad():
        seg_output = segmentation_model(image)[0]

    # Draw masks
    img_np = image_tensor.permute(1, 2, 0).cpu().numpy()
    plt.figure(figsize=(10, 10))
    plt.imshow(img_np)
    plt.axis('off')
    for mask in seg_output['masks']:
        plt.imshow(mask[0].cpu(), alpha=0.3, cmap='Reds')
    plt.title(f"Caption: {predicted_caption}", fontsize=12)
    plt.show()

import re
from collections import Counter

class Vocabulary:
    def __init__(self, freq_threshold):
        self.freq_threshold = freq_threshold
        self.itos = {0: "<pad>", 1: "<start>", 2: "<end>", 3: "<unk>"}
        self.stoi = {v: k for k, v in self.itos.items()}

    def __len__(self):
        return len(self.itos)

    def tokenizer(self, text):
        text = text.lower()
        text = re.sub(r'[^\w\s]', '', text)
        return text.split()

    def build_vocab(self, sentence_list):
        frequencies = Counter()
        idx = 4
        for sentence in sentence_list:
            tokens = self.tokenizer(sentence)
            frequencies.update(tokens)
            for token in tokens:
                if token not in self.stoi and frequencies[token] >= self.freq_threshold:
                    self.stoi[token] = idx
                    self.itos[idx] = token
                    idx += 1

    def numericalize(self, text):
        tokens = self.tokenizer(text)
        return [self.stoi.get(token, self.stoi["<unk>"]) for token in tokens]

def run_dual_caption_segmentation(image_tensor, caption_model, segmentation_model, vocab, max_len=20):
    caption_model.eval()
    segmentation_model.eval()
    image = image_tensor.unsqueeze(0).to(device)

    # Generate caption
    tgt = torch.tensor([[vocab.stoi["<start>"]]], dtype=torch.long).to(device)
    caption = []

    with torch.no_grad():
        for _ in range(max_len):
            output = caption_model(image, tgt)
            next_word = output.argmax(2)[:, -1].item()
            if next_word == vocab.stoi["<end>"]:
                break
            caption.append(vocab.itos.get(next_word, "<unk>"))
            tgt = torch.cat([tgt, torch.tensor([[next_word]], dtype=torch.long).to(device)], dim=1)

    caption_text = " ".join(caption)

    # Get segmentation masks
    with torch.no_grad():
        seg_output = segmentation_model(image)[0]

    # Visualize
    img_np = image_tensor.permute(1, 2, 0).cpu().numpy()
    plt.figure(figsize=(10, 10))
    plt.imshow(img_np)
    for mask in seg_output['masks']:
        plt.imshow(mask[0].cpu(), alpha=0.3, cmap='Reds')
    plt.title(f"Caption: {caption_text}", fontsize=12)
    plt.axis("off")
    plt.show()

